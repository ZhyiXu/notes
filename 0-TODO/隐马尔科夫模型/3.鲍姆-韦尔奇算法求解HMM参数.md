# 鲍姆-韦尔奇算法求解HMM参数

文章地址: https://www.cnblogs.com/pinard/p/6972299.html

　　在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问题之前，建议先阅读这个系列的前两篇以熟悉HMM模型和HMM的前向后向算法，以及[EM算法原理总结](http://www.cnblogs.com/pinard/p/6912636.html)，这些在本篇里会用到。在李航的《统计学习方法》中，这个算法的讲解只考虑了单个观测序列的求解，因此无法用于实际多样本观测序列的模型求解，本文关注于如何使用多个观测序列来求解HMM模型参数。

# 1. HMM模型参数求解概述

　　　　HMM模型参数求解根据已知的条件可以分为两种情况。

　　　　第一种情况较为简单，就是我们已知DD个长度为TT的观测序列和对应的隐藏状态序列，即{(O1,I1),(O2,I2),...(OD,ID)}{(O1,I1),(O2,I2),...(OD,ID)}是已知的，此时我们可以很容易的用最大似然来求解模型参数。

　　　　假设样本从隐藏状态qiqi转移到qjqj的频率计数是AijAij,那么状态转移矩阵求得为：A=[aij],其中aij=AijN∑s=1AisA=[aij],其中aij=Aij∑s=1NAis

　　　　假设样本隐藏状态为qjqj且观测状态为vkvk的频率计数是BjkBjk,那么观测状态概率矩阵为：B=[bj(k)],其中bj(k)=BjkM∑s=1BjsB=[bj(k)],其中bj(k)=Bjk∑s=1MBjs

　　　　假设所有样本中初始隐藏状态为qiqi的频率计数为C(i)C(i),那么初始概率分布为：Π=π(i)=C(i)N∑s=1C(s)Π=π(i)=C(i)∑s=1NC(s)

　　　　可见第一种情况下求解模型还是很简单的。但是在很多时候，我们无法得到HMM样本观察序列对应的隐藏序列，只有DD个长度为TT的观测序列，即{(O1),(O2),...(OD)}{(O1),(O2),...(OD)}是已知的，此时我们能不能求出合适的HMM模型参数呢？这就是我们的第二种情况，也是我们本文要讨论的重点。它的解法最常用的是鲍姆-韦尔奇算法，其实就是基于EM算法的求解，只不过鲍姆-韦尔奇算法出现的时代，EM算法还没有被抽象出来，所以我们本文还是说鲍姆-韦尔奇算法。

# 2. 鲍姆-韦尔奇算法原理

　　　　鲍姆-韦尔奇算法原理既然使用的就是EM算法的原理，那么我们需要在E步求出联合分布P(O,I|λ)P(O,I|λ)基于条件概率P(I|O,¯¯¯λ)P(I|O,λ¯)的期望，其中¯¯¯λλ¯为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数λλ。接着不停的进行EM迭代，直到模型参数的值收敛为止。

　　　　首先来看看E步，当前模型参数为¯¯¯λλ¯, 联合分布P(O,I|λ)P(O,I|λ)基于条件概率P(I|O,¯¯¯λ)P(I|O,λ¯)的期望表达式为：L(λ,¯¯¯λ)=∑IP(I|O,¯¯¯λ)logP(O,I|λ)L(λ,λ¯)=∑IP(I|O,λ¯)logP(O,I|λ)

　　　　在M步，我们极大化上式，然后得到更新后的模型参数如下：　¯¯¯λ=argmaxλ∑IP(I|O,¯¯¯λ)logP(O,I|λ)λ¯=argmaxλ∑IP(I|O,λ¯)logP(O,I|λ)

　　　　通过不断的E步和M步的迭代，直到¯¯¯λλ¯收敛。下面我们来看看鲍姆-韦尔奇算法的推导过程。

# 3. 鲍姆-韦尔奇算法的推导

　　　　我们的训练数据为{(O1,I1),(O2,I2),...(OD,ID)}{(O1,I1),(O2,I2),...(OD,ID)}，其中任意一个观测序列Od={o(d)1,o(d)2,...o(d)T}Od={o1(d),o2(d),...oT(d)},其对应的未知的隐藏状态序列表示为：Id={i(d)1,i(d)2,...i(d)T}Id={i1(d),i2(d),...iT(d)}

　　　　首先看鲍姆-韦尔奇算法的E步，我们需要先计算联合分布P(O,I|λ)P(O,I|λ)的表达式如下：P(O,I|λ)=D∏d=1πi(d)1bi(d)1(o(d)1)ai(d)1i(d)2bi(d)2(o(d)2)...ai(d)T−1i(d)Tbi(d)T(o(d)T)P(O,I|λ)=∏d=1Dπi1(d)bi1(d)(o1(d))ai1(d)i2(d)bi2(d)(o2(d))...aiT−1(d)iT(d)biT(d)(oT(d))

　　　　我们的E步得到的期望表达式为：L(λ,¯¯¯λ)=∑IP(I|O,¯¯¯λ)logP(O,I|λ)L(λ,λ¯)=∑IP(I|O,λ¯)logP(O,I|λ)

　　　　在M步我们要极大化上式。由于P(I|O,¯¯¯λ)=P(I,O|¯¯¯λ)/P(O|¯¯¯λ)P(I|O,λ¯)=P(I,O|λ¯)/P(O|λ¯),而P(O|¯¯¯λ)P(O|λ¯)是常数，因此我们要极大化的式子等价于：¯¯¯λ=argmaxλ∑IP(O,I|¯¯¯λ)logP(O,I|λ)λ¯=argmaxλ∑IP(O,I|λ¯)logP(O,I|λ)

　　　　我们将上面P(O,I|λ)P(O,I|λ)的表达式带入我们的极大化式子，得到的表达式如下：¯¯¯λ=argmaxλD∑d=1∑IP(O,I|¯¯¯λ)(logπi1+T−1∑t=1logait,it+1+T∑t=1logbit(ot))λ¯=argmaxλ∑d=1D∑IP(O,I|λ¯)(logπi1+∑t=1T−1logait,it+1+∑t=1Tlogbit(ot))

　　　　我们的隐藏模型参数λ=(A,B,Π)λ=(A,B,Π),因此下面我们只需要对上式分别对A,B,ΠA,B,Π求导即可得到我们更新的模型参数¯¯¯λλ¯　

 

　　　　首先我们看看对模型参数ΠΠ的求导。由于ΠΠ只在上式中括号里的第一部分出现，因此我们对于ΠΠ的极大化式子为：¯¯¯¯¯πi=argmaxπi1D∑d=1∑IP(O,I|¯¯¯λ)logπi1=argmaxπiD∑d=1N∑i=1P(O,i(d)1=i|¯¯¯λ)logπiπi¯=argmaxπi1∑d=1D∑IP(O,I|λ¯)logπi1=argmaxπi∑d=1D∑i=1NP(O,i1(d)=i|λ¯)logπi

　　　　由于πiπi还满足N∑i=1πi=1∑i=1Nπi=1，因此根据拉格朗日子乘法，我们得到πiπi要极大化的拉格朗日函数为：argmaxπiD∑d=1N∑i=1P(O,i(d)1=i|¯¯¯λ)logπi+γ(N∑i=1πi−1)argmaxπi∑d=1D∑i=1NP(O,i1(d)=i|λ¯)logπi+γ(∑i=1Nπi−1)

　　　　其中，γγ为拉格朗日系数。上式对πiπi求偏导数并令结果为0， 我们得到：D∑d=1P(O,i(d)1=i|¯¯¯λ)+γπi=0∑d=1DP(O,i1(d)=i|λ¯)+γπi=0

　　　　令ii分别等于从1到NN，从上式可以得到NN个式子，对这NN个式子求和可得：D∑d=1P(O|¯¯¯λ)+γ=0∑d=1DP(O|λ¯)+γ=0

　　　　从上两式消去γγ,得到πiπi的表达式为：πi=D∑d=1P(O,i(d)1=i|¯¯¯λ)D∑d=1P(O|¯¯¯λ)=D∑d=1P(O,i(d)1=i|¯¯¯λ)DP(O|¯¯¯λ)=D∑d=1P(i(d)1=i|O,¯¯¯λ)D=D∑d=1P(i(d)1=i|O(d),¯¯¯λ)Dπi=∑d=1DP(O,i1(d)=i|λ¯)∑d=1DP(O|λ¯)=∑d=1DP(O,i1(d)=i|λ¯)DP(O|λ¯)=∑d=1DP(i1(d)=i|O,λ¯)D=∑d=1DP(i1(d)=i|O(d),λ¯)D

　　　　利用我们在[隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率](http://www.cnblogs.com/pinard/p/6955871.html)里第二节中前向概率的定义可得：P(i(d)1=i|O(d),¯¯¯λ)=γ(d)1(i)P(i1(d)=i|O(d),λ¯)=γ1(d)(i)

　　　　因此最终我们在M步πiπi的迭代公式为：πi=D∑d=1γ(d)1(i)Dπi=∑d=1Dγ1(d)(i)D

 

　　　　现在我们来看看AA的迭代公式求法。方法和ΠΠ的类似。由于AA只在最大化函数式中括号里的第二部分出现，而这部分式子可以整理为：D∑d=1∑IT−1∑t=1P(O,I|¯¯¯λ)logait,it+1=D∑d=1N∑i=1N∑j=1T−1∑t=1P(O,i(d)t=i,i(d)t+1=j|¯¯¯λ)logaij∑d=1D∑I∑t=1T−1P(O,I|λ¯)logait,it+1=∑d=1D∑i=1N∑j=1N∑t=1T−1P(O,it(d)=i,it+1(d)=j|λ¯)logaij

　　　　由于aijaij还满足N∑j=1aij=1∑j=1Naij=1。和求解πiπi类似，我们可以用拉格朗日子乘法并对aijaij求导，并令结果为0，可以得到aijaij的迭代表达式为：aij=D∑d=1T−1∑t=1P(O(d),i(d)t=i,i(d)t+1=j|¯¯¯λ)D∑d=1T−1∑t=1P(O(d),i(d)t=i|¯¯¯λ)aij=∑d=1D∑t=1T−1P(O(d),it(d)=i,it+1(d)=j|λ¯)∑d=1D∑t=1T−1P(O(d),it(d)=i|λ¯)

　　　　利用[隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率](http://www.cnblogs.com/pinard/p/6955871.html)里第二节中前向概率的定义和第五节ξt(i,j)ξt(i,j)的定义可得们在M步aijaij的迭代公式为：aij=D∑d=1T−1∑t=1ξ(d)t(i,j)D∑d=1T−1∑t=1γ(d)t(i)aij=∑d=1D∑t=1T−1ξt(d)(i,j)∑d=1D∑t=1T−1γt(d)(i)

 

　　　　现在我们来看看BB的迭代公式求法。方法和ΠΠ的类似。由于BB只在最大化函数式中括号里的第三部分出现，而这部分式子可以整理为：D∑d=1∑IT∑t=1P(O,I|¯¯¯λ)logbit(ot)=D∑d=1N∑j=1T∑t=1P(O,i(d)t=j|¯¯¯λ)logbj(ot)∑d=1D∑I∑t=1TP(O,I|λ¯)logbit(ot)=∑d=1D∑j=1N∑t=1TP(O,it(d)=j|λ¯)logbj(ot)

　　　　由于bj(ot)bj(ot)还满足M∑k=1bj(ot=vk)=1∑k=1Mbj(ot=vk)=1。和求解πiπi类似，我们可以用拉格朗日子乘法并对bj(k)bj(k)求导，并令结果为0，得到bj(k)bj(k)的迭代表达式为：bj(k)=D∑d=1T∑t=1P(O,i(d)t=j|¯¯¯λ)I(o(d)t=vk)D∑d=1T∑t=1P(O,i(d)t=j|¯¯¯λ)bj(k)=∑d=1D∑t=1TP(O,it(d)=j|λ¯)I(ot(d)=vk)∑d=1D∑t=1TP(O,it(d)=j|λ¯)

　　　　其中I(o(d)t=vk)I(ot(d)=vk)当且仅当o(d)t=vkot(d)=vk时为1，否则为0. 利用[隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率](http://www.cnblogs.com/pinard/p/6955871.html)里第二节中前向概率的定义可得bj(ot)bj(ot)的最终表达式为：bj(k)=D∑d=1T∑t=1,o(d)t=vkγ(d)t(j)D∑d=1T∑t=1γ(d)t(j)bj(k)=∑d=1D∑t=1,ot(d)=vkTγt(d)(j)∑d=1D∑t=1Tγt(d)(j)

　　　　有了πi,aij,bj(k)πi,aij,bj(k)的迭代公式，我们就可以迭代求解HMM模型参数了。

# 4. 鲍姆-韦尔奇算法流程总结

　　　　这里我们概括总结下鲍姆-韦尔奇算法的流程。

　　　　输入： DD个观测序列样本{(O1),(O2),...(OD)}{(O1),(O2),...(OD)}

　　　　输出：HMM模型参数

　　　　1)随机初始化所有的πi,aij,bj(k)πi,aij,bj(k)

　　　　2) 对于每个样本d=1,2,...Dd=1,2,...D，用前向后向算法计算γ(d)t(i)，ξ(d)t(i,j),t=1,2...Tγt(d)(i)，ξt(d)(i,j),t=1,2...T

　　　　3)  更新模型参数：

πi=D∑d=1γ(d)1(i)Dπi=∑d=1Dγ1(d)(i)D

aij=D∑d=1T−1∑t=1ξ(d)t(i,j)D∑d=1T−1∑t=1γ(d)t(i)aij=∑d=1D∑t=1T−1ξt(d)(i,j)∑d=1D∑t=1T−1γt(d)(i)

bj(k)=D∑d=1T∑t=1,o(d)t=vkγ(d)t(j)D∑d=1T∑t=1γ(d)t(j)bj(k)=∑d=1D∑t=1,ot(d)=vkTγt(d)(j)∑d=1D∑t=1Tγt(d)(j)

　　　　4) 如果πi,aij,bj(k)πi,aij,bj(k)的值已经收敛，则算法结束，否则回到第2）步继续迭代。

　　　　以上就是鲍姆-韦尔奇算法的整个过程。