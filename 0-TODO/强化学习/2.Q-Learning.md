### 1. Q-Learning算法的引入　　　　

Q-Learning算法是一种使用==时序差分==求解强化学习控制问题的方法，回顾下此时我们的控制问题可以表示为：给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 即时奖励R，衰减因子γ, 探索率ϵ, 求解最优的动作价值函数q∗和最优策略π∗。

这一类强化学习的问题求解不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新策略，通过策略来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。

再回顾下时序差分法的控制问题，可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，比如我们上一篇讲到的SARSA, 而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。这一类的经典算法就是Q-Learning。

对于Q-Learning，我们会使用ϵ−贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的$ϵ−贪婪法$。这一点就是SARSA和Q-Learning本质的区别。

### 2. Q-Learning算法概述

​		Q-Learning算法的拓补图入下图所示：

![img](https://img2018.cnblogs.com/blog/1042406/201809/1042406-20180918202423478-583844904.jpg)

　　　　首先我们基于状态SS，用ϵ−ϵ−贪婪法选择到动作AA, 然后执行动作AA，得到奖励RR，并进入状态S′S′，此时，如果是SARSA，会继续基于状态S′S′，用ϵ−ϵ−贪婪法选择A′A′,然后来更新价值函数。但是Q-Learning则不同。

　　　　对于Q-Learning，它基于状态S′S′，没有使用ϵ−ϵ−贪婪法选择A′A′，而是使用贪婪法选择A′A′，也就是说，选择使Q(S′,a)Q(S′,a)最大的aa作为A′A′来更新价值函数。用数学公式表示就是：Q(S,A)=Q(S,A)+α(R+γmaxaQ(S′,a)−Q(S,A))Q(S,A)=Q(S,A)+α(R+γmaxaQ(S′,a)−Q(S,A))

　　　　对应到上图中就是在图下方的三个黑圆圈动作中选择一个使Q(S′,a)Q(S′,a)最大的动作作为A′A′。

　　　　此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态S′S′，用ϵ−ϵ−贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的A′A′会作为下一阶段开始时候的执行动作。

　　　　下面我们对Q-Learning算法做一个总结。

# 3. Q-Learning算法流程

　　　　下面我们总结下Q-Learning算法的流程。

　　　　算法输入：迭代轮数TT，状态集SS, 动作集AA, 步长αα，衰减因子γγ, 探索率ϵϵ,

　　　　输出：所有的状态和动作对应的价值QQ

　　　　1. 随机初始化所有的状态和动作对应的价值QQ. 对于终止状态其QQ值初始化为0.

　　　　2. for i from 1 to T，进行迭代。

　　　　　　a) 初始化S为当前状态序列的第一个状态。

　　　　　　b) 用ϵ−ϵ−贪婪法在当前状态SS选择出动作AA

　　　　　　c) 在状态SS执行当前动作AA,得到新状态S′S′和奖励RR

　　　　　　d)  更新价值函数Q(S,A)Q(S,A):Q(S,A)+α(R+γmaxaQ(S′,a)−Q(S,A))Q(S,A)+α(R+γmaxaQ(S′,a)−Q(S,A))

　　　　　　e) S=S′S=S′

　　　　　　f) 如果S′S′是终止状态，当前轮迭代完毕，否则转到步骤b)

# 4. Q-Learning算法实例：Windy GridWorld

　　　　我们还是使用和SARSA一样的例子来研究Q-Learning。如果对windy gridworld的问题还不熟悉，可以复习[强化学习（六）时序差分在线控制算法SARSA](https://www.cnblogs.com/pinard/p/9614290.html)第4节的第二段。

　　　　完整的代码参见我的github: https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py