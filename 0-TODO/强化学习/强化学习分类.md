# 强化学习分类

在学习强化学习的过程中我们经常会遇到一些陌生的名词分类，什么model-free，off-policy之类的，学习的时候不明觉厉可能就一眼带过了，但是其实这些分类的名词其实十分有趣，掌握他们十分有助于我们加深对相关强化学习算法的了解。

**1、Model-free 和 Model-based**

举个栗子：我们刚刚学习强化学习的时候都学习过grid-world这个机器人走迷宫的例子吧，就是有一个迷宫机器人从起点出发通过强化学习的方式选择出到达终点的最优路径。

![这里写图片描述](https://img-blog.csdn.net/20180621171907364?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

model-based方式就是我们给机器人地图全开，事先了解好整个游戏环境根据过往的经验选取最优策略，也就是说model-based他能通过想象来预判断接下来将要发生的所有情况. 然后选择这些想象情况中最好的那种. 并依据这种情况来采取下一步的策略

model-free方法就是不依赖模型，这种情况下就是直接将我们的机器人丢到迷宫里面瞎溜达，然后机器人会根据现实环境的反馈采取下一步的动作。这种方法不对环境进行建模也能找到最优的策略。Model-free 的方法有很多, 像 Q learning, Sarsa, Policy Gradients 都是从环境中得到反馈然后从中学习。

**2、Policy-based RL 和 Value-based RL**

说道Policy与Value就不得不提到他们的两大代表算法，Policy-based有 Policy Gradient；Value-based有Q-Learning。根据这两种算法我们很清晰的就能看出他们之间的区别，Policy-based算法是==通过对策略抽样训练出一个概率分布，并增强回报值高的动作被选中的概率==。而Value-based是通过==潜在奖励计算出==动作回报期望来作为选取动作的依据。

![这里写图片描述](https://img-blog.csdn.net/2018062117414024?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Policy基于策略的算法在连续动作空间上比起Value-based更有优势，还有一种NB的算法Actor-Critic他结合了这两类方法的优势之处, actor 会基于策略的概率分布做出动作，而 critic 会对做出的动作给出动作的价值, 这样就在原有的 policy gradients 上加速了学习过程。

**3、回合更新 和 单步更新**

回合更新和单步更新, 假设强化学习就是在玩游戏, 游戏回合有开始和结束. 回合更新指的是游戏开始后，我们要等到打完这一局我们才对这局游戏的经历进行总结学习新的策略。 而单步更新则是在游戏进行中每一步都在更新，这样就可以一边游戏一边学习不用等到回合结束。

再来说说方法, Monte-carlo learning 和基础版的 policy gradients 等都是回合更新制, Qlearning, Sarsa, 升级版的 policy gradients 等都是单步更新制.  因为单步更新更有效率, 所以现在大多方法都是基于单步更新。 比如有的强化学习问题并不属于回合问题。

**4、在线学习 和 离线学习**

On-Policy 在线学习智能体本身必须与环境进行互动然后一边选取动作一边学习。
Off-Policy 是指智能体可以亲自与环境进行交互进行学习，也可以通过别人的经验进行学习，也就是说经验是共享的，可以使自己的过往经验也可以是其他人的学习经验。

最典型的在线学习就是 Sarsa 了, 还有一种优化 Sarsa 的算法, 叫做 Sarsa lambda, 最典型的离线学习就是 Q-learning, 后来人也根据离线学习的属性, 开发了更强大的算法, 比如让计算机学会玩电动的 Deep-Q-Network.



# Q-Learning详解

**1、算法思想**

Q-Learning是强化学习算法中value-based的算法，Q即为Q(s,a) 就是在某一时刻的 s 状态下(s∈S)，采取动作a (a∈A) 动作能够获得收益的期望，环境会根据agent的动作反馈相应的回报reward r，所以算法的主要思想就是将State与Action构建成一张Q-table来存储Q值，然后根据Q值来选取能够获得最大的收益的动作。

| Q-Table | a1       | a2       |
| ------- | -------- | -------- |
| s1      | q(s1,a1) | q(s1,a2) |
| s2      | q(s2,a1) | q(s2,a2) |
| s3      | q(s3,a1) | q(s3,a2) |

**2、公式推导**

举个例子如图有一个GridWorld的游戏从起点出发到达终点为胜利掉进陷阱为失败。智能体（Agent）、环境状态（environment）、奖励（reward）、动作（action）可以将问题抽象成一个马尔科夫决策过程，我们在每个格子都算是一个状态 st , π(a|s)在s状态下采取动作a策略 。 P(s’|s,a)也可以写成 $P_{ss'}^a$ 为在s状态下选择a动作转换到下一个状态s’ 的概率。R(s’|s,a)表示在s状态下采取a动作转移到s’的奖励reward，我们的目的很明确就是找到一条能够到达终点获得最大奖赏的策略。

![这里写图片描述](https://img-blog.csdn.net/20180619195404915?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

所以目标就是求出累计奖励最大的策略的期望：

Goal:  $max_\pi E [\sum_{t=0}^H \gamma^t R(S_t,A_t,S_{t+1}) | \pi]$

Q-learning的主要优势就是使用了==时间差分法TD==（融合了==蒙特卡洛==和==动态规划==）能够进行离线学习, 使用==bellman方程==可以对==马尔科夫过程==求解最优策略。

**贝尔曼方程**

通过bellman方程求解马尔科夫决策过程的最佳决策序列，状态值函数 Vπ(s) 可以评价当前状态的好坏，每个状态的值不仅由当前状态决定还要由后面的状态决定，所以状态的累计奖励求期望就可得出当前s的状态值函数V(s)。bellman方程如下：

$V_π(s) = E(U_t|S_t = s)$ 


$V_π(s) = E_π[R_{t+1}+γ[R_{t+2} + γ[.......]]|S_t = s]$

$V_π(s) = E_π[R_{t+1}+γV(s')|S_t = s]$

最优累计期望可用$V^*(s) $表示，可知最优值函数就是 

$V^*(s)=max_πV_\pi(s)$

$V^*(s)=\max_πE[\sum_{t=0}^{H}γ^tR(S_t,A_t,S_{t+1}) | π,s_0=s]$

**Q(s,a) 状态动作值函数**

$q_π(s,a) = E_π[r_{t+1}+γr_{t+2}+γ^2r_{t+3}+....|A_t=a,S_t=s] $ 

 其中 $G_t$ 是t时刻开始的总折扣奖励，从这里我们能看出来 γ 衰变值对 Q 函数的影响，γ 越接近于1代表它越有远见会着重考虑后续状态的的价值，当 γ 接近0的时候就会变得近视只考虑当前的利益的影响。所以从0到1，算法就会越来越会考虑后续回报的影响。

$q_π(s,a) = E_π[R_{t+1}+γq_π(S_{t+1},A_{t+1})|A_t=a,S_t=s]$

最优价值动作函数 $Q^*(s,a)=max_\pi Q^*(s,a)$

打开期望如下

$ Q^*(s,a)=\sum_{s'} P(s'|s,a)(R(s,a,s')+γ\max_{a'}Q^*(s',a')) $

==Bellman方程实际上就是价值动作函数的转换关系:== 

$V_π(s) = \sum_{a∈A}π(a|s)q_π(s,a) $

$q_π(s,a) = R_s^a + γ\sum_{s'∈S}P_{ss'}^aV_π(s') $

$V_π(s)=\sum_{a'∈A}π(a|s)[R_s^a+γ\sum_{s'}P_{ss'}^aV_π(s')]$

![Q值迭代公式](https://img-blog.csdn.net/20180619205906929?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

根据下图更直观的了解 V(s) 与 Q(s,a) 的关系

![V(s)与Q(s,a)的关系](https://img-blog.csdn.net/20180619210234678?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

时间差分法： https://blog.csdn.net/qq_30615903/article/details/80821061 

时间差分方法结合了蒙特卡罗的采样方法和动态规划方法的bootstrapping ( 利用后继状态的值函数估计当前值函数 ) 使得他可以适用于model-free的算法并且是单步更新，速度更快。值函数计算方式如下

$V(s)←V(s)+\alpha (R_{t+1}+\gamma V(s')-V(s))$

其中$R_{t+1}+\gamma V(s')$ 被称为TD目标，$\delta_t=R_{t+1}+\gamma V(s')-V(s)$ 称为 TD 偏差。

**3、更新公式**

根据以上推导可以对Q值进行计算，所以有了Q值我们就可以进行学习，也就是Q-table的更新过程，其中α为学习率γ为奖励性衰变系数，采用时间差分法的方法进行更新。

$Q(s,a) ← Q(s,a) + α[r + γmax_{a'}Q(s',a')-Q(s,a)] $ 

上式就是Q-learning更新的公式，根据下一个状态s’中选取最大的Q(s′,a′) 值乘以衰变 γ 加上真实回报值最为Q现实，而根据过往Q表里面的Q(s,a) 作为 Q估计。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190225164323939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz,size_16,color_FFFFFF,t_70)

![这里写图片描述](https://img-blog.csdn.net/20180615180722209?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**4、实现代码**

代码来自网上各路大神的源码，非原创，据反映没图片跑不通，所以建了个github，https://github.com/xshura/reinforcement_learning



# Policy Gradient 算法详解

**1、算法思想**

之前的Q-Learning DQN Sarsa 都是通过计算动作得分来决策的，我们是在==确定了价值函数的基础上==采用某种策略（贪婪-epsilon）的方式去选取动作。经过学习发现 Policy Gradient 并非我预料中的策略迭代，这种策略梯度的算法归类于策略优化算法中，而不是以迭代为基础的动态规划算法。

![这里写图片描述](https://img-blog.csdn.net/20180620163024604?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Policy Gradient不通过误差反向传播，它通过观测信息选出一个行为直接进行反向传播，当然出人意料的是他并没有误差，而是==利用reward奖励直接对选择行为的可能性进行增强和减弱==，好的行为会被增加下一次被选中的概率，不好的行为会被减弱下次被选中的概率。

举例如下图所示：输入当前的状态，输出action的概率分布，选择概率最大的一个action作为要执行的操作。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181210114307832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz,size_16,color_FFFFFF,t_70)	

而一个完整的策略 τ 代表的是一整个回合中，对于每个状态下所采取的的动作所构成的序列，而每个回合episode中每个动作的回报和等于一个回合的回报值 $R =\sum_{t=1}^Tr_t$

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181210135748872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz,size_16,color_FFFFFF,t_70)

通过以上可得知 π 在参数为 θ 情况下时 τ 发生的概率：
得到了概率之后我们就可以根据采样得到的回报值计算出数学期望，从而得到目标函数，然后用来更新我们的参数 θ


优点：

- 连续的动作空间（或者高维空间）中更加高效；
- 可以实现随机化的策略；
- 某种情况下，价值函数可能比较难以计算，而策略函数较容易。

缺点：

- 通常收敛到局部最优而非全局最优
- 评估一个策略通常低效（这个过程可能慢，但是具有更高的可变性，其中也会出现很多并不有效的尝试，而且方差高）

**2、公式推导**
在本篇文章将会使用参数θ逼近拟合状态值函数$V_π(s)$和 状态动作值函数 $Q_π(s,a)$可以理解为我们使用比如神经网络的的function来逼近拟合状态值函数$V_π(s)$ 和 状态动作值函数 $Q_π(s,a)$的分布。
$V_θ(s)≈V_π(s)$ 

$Q_θ(s,a)≈Q_π(s,a)$

 并且将策略Policy参数化为πθ(s,a)=P(a∣s,θ)π_\theta(s,a) = P(a|s,\theta)π 
θ
	
 (s,a)=P(a∣s,θ)，使用model-free的方法，不借助与agent做决策而是将agent丢入不确定的动态环境下，不提供动态环境的信息，让agent自己瞎溜达根据所获得的策略信息更新参数。
通常情况下目标策略有三种方式

使用初始价值来判断：J1(θ)=Vπθ(s1)=Eπθ[v1]J_1(θ)=V^{π_θ}(s1)=E_{π_θ}[v_1]J 
1
	
 (θ)=V 
π 
θ
	

 (s1)=E 
π 
θ
	


 [v 
1
	
 ]
使用平均价值：JavV(θ)=∑sdπθ(s)Vπθ(s)J_{av}V(θ)=∑_sd^{π_θ}(s)V^{π_θ}(s)J 
av
	
 V(θ)=∑ 
s
	
 d 
π 
θ
	

 (s)V 
π 
θ
	

 (s)
使用每次time-step的平均奖励：JavR(θ)=∑sdπθ(s)∑aπθ(s,a)RasJ_{av}R(θ)=∑_sd^{π_θ}(s)∑_aπ_θ(s,a)R^a_sJ 
av
	
 R(θ)=∑ 
s
	
 d 
π 
θ
	

 (s)∑ 
a
	
 π 
θ
	
 (s,a)R 
s
a
	

我们暂定使用初始值法做目标策略

J(θ)=Vπθ(s1)=Eπθ[v1]=E(r1+γr2+γ2r3+.......∣πθ)J(\theta)=V^{π_θ}(s1)=E_{π_θ}[v_1]=E(r_1+\gamma r_2 +\gamma^2r_3+.......|\pi_\theta)J(θ)=V 
π 
θ
	

 (s1)=E 
π 
θ
	


 [v 
1
	
 ]=E(r 
1
	
 +γr 
2
	
 +γ 
2
 r 
3
	
 +.......∣π 
θ
	
 )

对该目标函数进行最大化也就是在搜索一组参数向量 θ\thetaθ ，使得目标函数最大。这实际上做的是改变策略概率而非改变行动轨迹的工作，所以根据套路我们接下来就要使用梯度下降求解∇θJ(θ)∇_θJ(θ)∇ 
θ
	
 J(θ)

∇θJ⎛⎝⎜⎜⎜⎜θ⎞⎠⎟⎟⎟⎟=⎛⎝⎜⎜⎜⎜∂J(θ)∂θ1⋮∂J(θ)∂θn⎞⎠⎟⎟⎟⎟∇_θJ(θ) = \begin{pmatrix}\frac{∂J(θ)}{∂θ_1 }\\ ⋮ \\\frac{∂J(θ)}{∂θ_n }\end{pmatrix}∇ 
θ
	
 J(θ)= 
⎝
⎜
⎜
⎛
	

∂θ 
1
	

∂J(θ)
	

⋮
∂θ 
n
	

∂J(θ)
	

​	

⎠
⎟
⎟
⎞
	

在连续策略上选用Gaussian Policy在离散策略下采用softmax Policy

策略梯度定理（The policy gradient theorem）
由于我们是基于model-free的所以无法事先知道动态环境的状态分布，而奖励函数有依赖于动作和状态分布，所以无法进行求导，所以我们需要把奖励采用无偏估计的方法计算出来，首先随机采样然后取均值来估计

假设一个只有一步的MDP，对它使用策略梯度下降。πθ(s,a)π_θ(s,a)π 
θ
	
 (s,a)表示关于参数θ的函数，映射是P(a∣s,θ)P(a|s,θ)P(a∣s,θ)。它在状态s所执行一a动作的奖励即为r=R(s,a)r=R(s,a)r=R(s,a)。那么选择行动a的奖励为πθ(s,a)Rs,aπ_θ(s,a)R_{s,a}π 
θ
	
 (s,a)R 
s,a
	
 ，在状态 s 的加权回报为∑a∈Aπθ(s,a)R(s,a)∑_{a∈A}π_θ(s,a)R(s,a)∑ 
a∈A
	
 π 
θ
	
 (s,a)R(s,a)，推导如下

J(θ)=Eπθ[R(s,a)]J(θ)=E_{π_θ}[R(s,a)]J(θ)=E 
π 
θ
	


 [R(s,a)]
J(θ)=∑s∈Sd(s)∑a∈Aπθ(s,a)R(s,a)J(θ)=∑_{s∈S}d(s)∑_{a∈A}π_θ(s,a)R(s,a)J(θ)=∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 π 
θ
	
 (s,a)R(s,a)

梯度为：
∇θJ(θ)=∇θ∑s∈Sd(s)∑a∈Aπθ(s,a)R(s,a)∇_θJ(θ)=∇_θ∑_{s∈S}d(s)∑_{a∈A}π_θ(s,a)R(s,a)∇ 
θ
	
 J(θ)=∇ 
θ
	
 ∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 π 
θ
	
 (s,a)R(s,a)
∇θJ(θ)=∑s∈Sd(s)∑a∈A∇θπθ(s,a)R(s,a)∇_θJ(θ)=∑_{s∈S}d(s)∑_{a∈A}∇_θπ_θ(s,a)R(s,a)∇ 
θ
	
 J(θ)=∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 ∇ 
θ
	
 π 
θ
	
 (s,a)R(s,a)

假设策略πθπ_θπ 
θ
	
 为零的时候可微，并且已知梯度∇θπθ(s,a)∇_θπ_θ(s,a)∇ 
θ
	
 π 
θ
	
 (s,a)，应用似然比(likelihood ratio)的技巧定义∇θlogπθ(s,a)∇_θlogπ_θ(s,a)∇ 
θ
	
 logπ 
θ
	
 (s,a)为得分函数（score function）。二者关系如下：
∵∇θπθ(s,a)=πθ(s,a)∇θπθ(s,a)πθ(s,a)=πθ(s,a)∇θlogπθ(s,a)∇_θπ_θ(s,a)=π_θ(s,a)\frac{∇_θπ_θ(s,a)}{π_θ(s,a)}=π_θ(s,a)∇_θlogπ_θ(s,a)∇ 
θ
	
 π 
θ
	
 (s,a)=π 
θ
	
 (s,a) 
π 
θ
	
 (s,a)
∇ 
θ
	
 π 
θ
	
 (s,a)
	
 =π 
θ
	
 (s,a)∇ 
θ
	
 logπ 
θ
	
 (s,a)

∴∇θJ(θ)=∑s∈Sd(s)∑a∈Aπθ(s,a)∇θlogπθ(s,a)R(s,a)∇_θJ(θ)=∑_{s∈S}d(s)∑_{a∈A}π_θ(s,a)∇_θlogπ_θ(s,a)R(s,a)∇ 
θ
	
 J(θ)=∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 π 
θ
	
 (s,a)∇ 
θ
	
 logπ 
θ
	
 (s,a)R(s,a)

根据上文d(s)是策略中的状态分布，π(s,a)\pi(s,a)π(s,a)是当前状态的动作概率分布，所以可将策略梯度恢复成期望形式：

∇θJ(θ)=Eπθ[∇θlogπθ(s,a)R(s,a)]∇_θJ(θ)=E_{π_θ}[∇_θlogπ_θ(s,a)R(s,a)]∇ 
θ
	
 J(θ)=E 
π 
θ
	


 [∇ 
θ
	
 logπ 
θ
	
 (s,a)R(s,a)]

然后再将似然率方式的策略梯度方法应用到多步MDPs上，此时的因为回报值应该为过程中的多步回报值之和，在这里使用Qπ(s,a)Q_π(s,a)Q 
π
	
 (s,a)代替奖励值R单步(s,a)，对于任意可微的策略梯度如下（策略价值计算公式）：

∇θJ(θ)=Eπθ[∇θlogπθ(s,a)Qπθ(s,a)]∇_θJ(θ)=E_{π_θ}[∇_θlogπ_θ(s,a)Q^{π_θ}(s,a)]∇ 
θ
	
 J(θ)=E 
π 
θ
	


 [∇ 
θ
	
 logπ 
θ
	
 (s,a)Q 
π 
θ
	

 (s,a)]
策略梯度定理详细推导过程如下


蒙特卡洛梯度策略强化算法Monte-Carlo Policy Gradient
(不带基数)蒙特卡洛策略梯度通过情节采样，使用随机梯度上升法更新参数，使用策略梯度法，返回vtv_tv 
t
	
 作为Qπθ(st,at)的无偏估计Q^{π_θ}(s_t,a_t)的无偏估计Q 
π 
θ
	

 (s 
t
	
 ,a 
t
	
 )的无偏估计

Δθt=α∇θlogπθ(st,at)vtΔθ_t=α∇_θlogπ_θ(s_t,a_t)v_tΔθ 
t
	
 =α∇ 
θ
	
 logπ 
θ
	
 (s 
t
	
 ,a 
t
	
 )v 
t
	

所以可得更新公式

θt+1←θ+α∇θlogπθ(st,at)vt\theta_{t+1} ← \theta + \alpha∇_θlogπ_θ(s_t,a_t)v_tθ 
t+1
	
 ←θ+α∇ 
θ
	
 logπ 
θ
	
 (s 
t
	
 ,a 
t
	
 )v 
t
	

算法伪代码如下


带基数的蒙特卡洛梯度策略强化算法ERINFORCE with baseline
在某些情况下可能会出现每一个动作的回报值都是正数但是由于我们是通过采样的方式进行更新的所以这时候可以引入一个基数b
则原式需要修改为
∇θJ(θ)=Eπθ[∇θlogπθ(s,a)[Qπθ(s,a)−b(s)]]∇_θJ(θ)=E_{π_θ}[∇_θlogπ_θ(s,a)[Q^{π_θ}(s,a)-b(s)]]∇ 
θ
	
 J(θ)=E 
π 
θ
	


 [∇ 
θ
	
 logπ 
θ
	
 (s,a)[Q 
π 
θ
	

 (s,a)−b(s)]]
————————————————
版权声明：本文为CSDN博主「shura_R」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_30615903/java/article/details/80747380



# Actor-Critic算法详解

**Actor-Critic详解**

之前在强化学习分类中，我们提到了Policy-based与Value-based两种方式，然而有一种算法合并了Value-based (比如 Q learning) 和 Policy-based ( 比如 Policy Gradients) 两类强化学习算法，就是Actor-Critic方法。

**1、算法思想**

Actor-Critic算法分为两部分，我们分开来看actor的前身是policy gradient他可以轻松地在连续动作空间内选择合适的动作，value-based的Q-learning做这件事就会因为空间过大而爆炸，但是又因为Actor是基于回合更新的所以学习效率比较慢，这时候我们发现可以使用一个value-based的算法作为Critic就可以实现单步更新。这样两种算法相互补充就形成了我们的Actor-Critic。

![这里写图片描述](https://img-blog.csdn.net/20180622105249751?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率。

- Actor-Critic优点：可以进行单步更新, 相较于传统的PG回合更新要快.
- Actor-Critic缺点：Actor的行为取决于 Critic 的Value，但是因为 Critic本身就很难收敛和Actor一起更新的话就更难收敛了。

（为了解决收敛问题， Deep-mind 提出了 Actor-Critic 升级版 Deep Deterministic Policy Gradient，后者融合了DQN 的优势, 解决了收敛难的问题，之后我会详细解释这种算法）

**2、公式推导**

Actor（玩家）：为了玩转这个游戏得到尽量高的reward，需要一个策略：输入state，输出action，即上面的第2步。（可以用神经网络来近似这个函数。剩下的任务就是如何训练神经网络，得更高的reward。这个网络就被称为actor）

Critic（评委）：因为actor是基于策略policy的所以需要critic来计算出对应actor的value来反馈给actor，告诉他表现得好不好。所以就要使用到之前的Q值。（当然这个Q-function所以也可以用神经网络来近似。这个网络被称为critic。)

![这里写图片描述](https://img-blog.csdn.net/20180622111424899?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjE1OTAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

再提一下之前用过的符号

策略π(s)表示了agent的action，其输出是单个的action动作，而是选择动作的概率分布，所以一个状态下的所有动作概率加和应当为 1

π(a∣s)表示策略

首先来看Critic的策略值函数即策略π\piπ的

Vπ(s)V_\pi(s)V 
 (s)具体推导方式请参考之前Qlearning的推导方式

Vπ(s)=Eπ[r+γVπ(s′)]V_\pi(s)=E_\pi[r + \gamma V_\pi(s&#x27;)]V 
π
	
 (s)=E 
π
	
 [r+γV 
π
	
 (s 
′
 )]

策略的动作值函数如下

Qπ(s,a)=Ras+γVπ(s')Q_π(s,a)=R^a_s+γV_π(s′)Q 
π
	
 (s,a)=R 
s
a
	
 +γV 
π
	
 (s′)

此处提出了优势函数A，优势函数表示在状态 s 下，选择动作 a 有多好。如果 action a 比 average 要好，那么，advantage function 就是 positive 的，否则，就是 negative 的。

Aπ(s,a)=Qπ(s,a)−Vπ(s)=r+γVπ(s')−Vπ(s)A_\pi(s,a)=Q_π(s,a)-V_\pi(s)= r + γV_π(s′) -V_\pi(s)A 
π
	
 (s,a)=Q 
π
	
 (s,a)−V 
π
	
 (s)=r+γV 
π
	
 (s′)−V 
π
	
 (s)

前置条件下面两个公式是等价的因为使用了likelihood ratio似然比的方法，所以可能各种资料写的不一样，大家注意不要被搞蒙了

∇θπθ(s,a)=πθ(s,a)∇θπθ(s,a)πθ(s,a)=πθ(s,a)∇θlogπθ(s,a)∇_θπ_θ(s,a)=π_θ(s,a)\frac{∇_θπ_θ(s,a)}{π_θ(s,a)}=π_θ(s,a)∇_θlogπ_θ(s,a)∇ 
θ
	
 π 
θ
	
 (s,a)=π 
θ
	
 (s,a) 
π 
θ
	
 (s,a)
∇ 
θ
	
 π 
θ
	
 (s,a)
	
 =π 
θ
	
 (s,a)∇ 
θ
	
 logπ 
θ
	
 (s,a)

接下来我们看Actor，这部分假设采取policy Gradient这样的话使用策略梯度定理

∇θJ(θ)=∑s∈Sd(s)∑a∈Aπθ(s,a)∇θlogπ(a∣s;θ)Qπ(s,a)∇_θJ(θ)=∑_{s∈S}d(s)∑_{a∈A}π_θ(s,a)∇_θlogπ(a|s;\theta)Q_{π}(s,a)∇ 
θ
	
 J(θ)=∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 π 
θ
	
 (s,a)∇ 
θ
	
 logπ(a∣s;θ)Q 
π
	
 (s,a)
∇θJ(θ)=Eπθ[∇θlogπθ(s,a)Qπθ(s,a)]∇_θJ(θ)=E_{π_θ}[∇_θlogπ_θ(s,a)Q_{π_θ}(s,a)]∇ 
θ
	
 J(θ)=E 
π 
θ
	


 [∇ 
θ
	
 logπ 
θ
	
 (s,a)Q 
π 
θ
	


 (s,a)]

此处将Qπ(s,a)Q_{π}(s,a)Q 
π
	
 (s,a)换成上文提到的Aπ(s,a)A_\pi(s,a)A 
π
	
 (s,a)
∇θJ(θ)=∑s∈Sd(s)∑a∈Aπθ(s,a)∇θlogπ(a∣s;θ)Aπ(s,a)∇_θJ(θ)=∑_{s∈S}d(s)∑_{a∈A}π_θ(s,a)∇_θlogπ(a|s;\theta)A_{\pi}(s,a)∇ 
θ
	
 J(θ)=∑ 
s∈S
	
 d(s)∑ 
a∈A
	
 π 
θ
	
 (s,a)∇ 
θ
	
 logπ(a∣s;θ)A 
π
	
 (s,a)
∇θJ(θ)=Eπθ[∇θlogπθ(s,a)Aπθ(s,a)]∇_θJ(θ)=E_{π_θ}[∇_θlogπ_θ(s,a)A_{\pi_θ}(s,a)]∇ 
θ
	
 J(θ)=E 
π 
θ
	


 [∇ 
θ
	
 logπ 
θ
	
 (s,a)A 
π 
θ
	


 (s,a)]
更新公式如下
下面几种形式都是一样的所以千万不要蒙
θt+1←θt+αAπθ(s,a)∇θlogπθ(s,a)\theta_{t+1}←\theta_t+\alpha A_{\pi_θ}(s,a) ∇_θlogπ_θ(s,a)θ 
t+1
	
 ←θ 
t
	
 +αA 
π 
θ
	


 (s,a)∇ 
θ
	
 logπ 
θ
	
 (s,a)
θt+1←θt+αAπθ(s,a)∇θπ(At∣St,θ)π(At∣St,θ)\theta_{t+1}←\theta_t+\alpha A_{\pi_θ}(s,a)\frac{∇_θπ(A_t|S_t,\theta)}{π(A_t|S_t,\theta)}θ 
t+1
	
 ←θ 
t
	
 +αA 
π 
θ
	


 (s,a) 
π(A 
t
	
 ∣S 
t
	
 ,θ)
∇ 
θ
	
 π(A 
t
	
 ∣S 
t
	
 ,θ)
	

θt+1←θt+α(r+γVπ(st+1)−Vπ(st))∇θπ(At∣St,θ)π(At∣St,θ)\theta_{t+1}←\theta_t+\alpha (r + γV_π(s_{t+1}) -V_\pi(s_t))\frac{∇_θπ(A_t|S_t,\theta)}{π(A_t|S_t,\theta)}θ 
t+1
	
 ←θ 
t
	
 +α(r+γV 
π
	
 (s 
t+1
	
 )−V 
π
	
 (s 
t
	
 )) 
π(A 
t
	
 ∣S 
t
	
 ,θ)
∇ 
θ
	
 π(A 
t
	
 ∣S 
t
	
 ,θ)
	

损失函数如下
A可看做是常数所以可以求和平均打开期望，又因为损失函数要求最小所以加个"-"变换为解最小的问题
Actor：Lπ=−1n∑ni=1Aπ(s,a)logπ(s,a)L_\pi =-\frac{1}{n}\sum_{i=1}^nA_{\pi}(s,a) logπ(s,a )L 
π
	
 =− 
n
1
	
 ∑ 
i=1
n
	
 A 
π
	
 (s,a)logπ(s,a)
值迭代可以直接使用均方误差MSE作为损失函数
Critic：Lv=1n∑ni=1e2iL_v = \frac{1}{n}\sum_{i=1}^ne_i^2L 
v
	
 = 
n
1
	
 ∑ 
i=1
n
	
 e 
i
2
	


n-step的伪代码如下


3、代码实现
import numpy as np
import tensorflow as tf
import gym

np.random.seed(2)
tf.set_random_seed(2)  # reproducible

# 超参数
OUTPUT_GRAPH = False
MAX_EPISODE = 3000
DISPLAY_REWARD_THRESHOLD = 200  # 刷新阈值
MAX_EP_STEPS = 1000   		    #最大迭代次数
RENDER = False  # 渲染开关
GAMMA = 0.9     # 衰变值
LR_A = 0.001    # Actor学习率
LR_C = 0.01     # Critic学习率

env = gym.make('CartPole-v0')
env.seed(1)  
env = env.unwrapped

N_F = env.observation_space.shape[0] # 状态空间
N_A = env.action_space.n		     # 动作空间


class Actor(object):
    def __init__(self, sess, n_features, n_actions, lr=0.001):
        self.sess = sess

        self.s = tf.placeholder(tf.float32, [1, n_features], "state")
        self.a = tf.placeholder(tf.int32, None, "act")
        self.td_error = tf.placeholder(tf.float32, None, "td_error")  # TD_error
    
        with tf.variable_scope('Actor'):
            l1 = tf.layers.dense(
                inputs=self.s,
                units=20,    # number of hidden units
                activation=tf.nn.relu,
                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='l1'
            )
    
            self.acts_prob = tf.layers.dense(
                inputs=l1,
                units=n_actions,    # output units
                activation=tf.nn.softmax,   # get action probabilities
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='acts_prob'
            )
    
        with tf.variable_scope('exp_v'):
            log_prob = tf.log(self.acts_prob[0, self.a])
            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss
    
        with tf.variable_scope('train'):
            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)
    
    def learn(self, s, a, td):
        s = s[np.newaxis, :]
        feed_dict = {self.s: s, self.a: a, self.td_error: td}
        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)
        return exp_v
    
    def choose_action(self, s):
        s = s[np.newaxis, :]
        probs = self.sess.run(self.acts_prob, {self.s: s})   # 获取所有操作的概率
        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int


class Critic(object):
    def __init__(self, sess, n_features, lr=0.01):
        self.sess = sess

        self.s = tf.placeholder(tf.float32, [1, n_features], "state")
        self.v_ = tf.placeholder(tf.float32, [1, 1], "v_next")
        self.r = tf.placeholder(tf.float32, None, 'r')
    
        with tf.variable_scope('Critic'):
            l1 = tf.layers.dense(
                inputs=self.s,
                units=20,  # number of hidden units
                activation=tf.nn.relu,  # None
                # have to be linear to make sure the convergence of actor.
                # But linear approximator seems hardly learns the correct Q.
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='l1'
            )
    
            self.v = tf.layers.dense(
                inputs=l1,
                units=1,  # output units
                activation=None,
                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights
                bias_initializer=tf.constant_initializer(0.1),  # biases
                name='V'
            )
    
        with tf.variable_scope('squared_TD_error'):
            self.td_error = self.r + GAMMA * self.v_ - self.v
            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval
        with tf.variable_scope('train'):
            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)
    
    def learn(self, s, r, s_):
        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]
    
        v_ = self.sess.run(self.v, {self.s: s_})
        td_error, _ = self.sess.run([self.td_error, self.train_op],
                                          {self.s: s, self.v_: v_, self.r: r})
        return td_error

sess = tf.Session()
actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)  # 初始化Actor
critic = Critic(sess, n_features=N_F, lr=LR_C)     # 初始化Critic
sess.run(tf.global_variables_initializer())        # 初始化参数

if OUTPUT_GRAPH:
    tf.summary.FileWriter("logs/", sess.graph)        # 输出日志

# 开始迭代过程 对应伪代码部分
for i_episode in range(MAX_EPISODE):
    s = env.reset() # 环境初始化
    t = 0
    track_r = []    # 每回合的所有奖励
    while True:
        if RENDER: env.render()
        a = actor.choose_action(s)       # Actor选取动作
        s_, r, done, info = env.step(a)   # 环境反馈
        if done: r = -20    # 回合结束的惩罚
        track_r.append(r)  # 记录回报值r
        td_error = critic.learn(s, r, s_)  # Critic 学习
        actor.learn(s, a, td_error)        # Actor 学习
        s = s_
        t += 1

        if done or t >= MAX_EP_STEPS:
            # 回合结束, 打印回合累积奖励
            ep_rs_sum = sum(track_r)
            if 'running_reward' not in globals():
                running_reward = ep_rs_sum
            else:
                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05
            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering
            print("episode:", i_episode, "  reward:", int(running_reward))
            break
 